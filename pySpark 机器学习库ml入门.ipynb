{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c266f2b8",
   "metadata": {},
   "source": [
    "使用ml中的函数方法来预测婴儿生存率，\n",
    "数据来源 http://www.tomdrabas.com/data/LearningPySpark/births_transformed.csv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cd6dffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.types as typ\n",
    "from pyspark.ml import Pipeline\n",
    "import pyspark.ml.classification as cl\n",
    "import pyspark.ml.evaluation as ev\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pyspark.sql import SparkSession,SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c14f66bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession\\\n",
    "    .builder \\\n",
    "    .appName(\"births_transformed\") \\\n",
    "    .config(\"spark.some.config.option\", \"setting\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d78c2f",
   "metadata": {},
   "source": [
    "#  1 分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28ee9c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+-----------+----------------+------------------+----------+---------+---------+---------+----------------+-----------------+----------------------+------------------+------------+-------------+------------+-------------+------------------+\n",
      "|INFANT_ALIVE_AT_REPORT|BIRTH_PLACE|MOTHER_AGE_YEARS|FATHER_COMBINE_AGE|CIG_BEFORE|CIG_1_TRI|CIG_2_TRI|CIG_3_TRI|MOTHER_HEIGHT_IN|MOTHER_PRE_WEIGHT|MOTHER_DELIVERY_WEIGHT|MOTHER_WEIGHT_GAIN|DIABETES_PRE|DIABETES_GEST|HYP_TENS_PRE|HYP_TENS_GEST|PREV_BIRTH_PRETERM|\n",
      "+----------------------+-----------+----------------+------------------+----------+---------+---------+---------+----------------+-----------------+----------------------+------------------+------------+-------------+------------+-------------+------------------+\n",
      "|                     0|          1|              29|                99|         0|        0|        0|        0|              99|              999|                   999|                99|           0|            0|           0|            0|                 0|\n",
      "|                     0|          1|              22|                29|         0|        0|        0|        0|              65|              180|                   198|                18|           0|            0|           0|            0|                 0|\n",
      "|                     0|          1|              38|                40|         0|        0|        0|        0|              63|              155|                   167|                12|           0|            0|           0|            0|                 0|\n",
      "+----------------------+-----------+----------------+------------------+----------+---------+---------+---------+----------------+-----------------+----------------------+------------------+------------+-------------+------------+-------------+------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "labels = [('INFANT_ALIVE_AT_REPORT', typ.IntegerType()),\n",
    "          ('BIRTH_PLACE', typ.StringType()),\n",
    "          ('MOTHER_AGE_YEARS', typ.IntegerType()),\n",
    "          ('FATHER_COMBINE_AGE', typ.IntegerType()),\n",
    "          ('CIG_BEFORE', typ.IntegerType()),\n",
    "          ('CIG_1_TRI', typ.IntegerType()),\n",
    "          ('CIG_2_TRI', typ.IntegerType()),\n",
    "          ('CIG_3_TRI', typ.IntegerType()),\n",
    "          ('MOTHER_HEIGHT_IN', typ.IntegerType()),\n",
    "          ('MOTHER_PRE_WEIGHT', typ.IntegerType()),\n",
    "          ('MOTHER_DELIVERY_WEIGHT', typ.IntegerType()),\n",
    "          ('MOTHER_WEIGHT_GAIN', typ.IntegerType()),\n",
    "          ('DIABETES_PRE', typ.IntegerType()),\n",
    "          ('DIABETES_GEST', typ.IntegerType()),\n",
    "          ('HYP_TENS_PRE', typ.IntegerType()),\n",
    "          ('HYP_TENS_GEST', typ.IntegerType()),\n",
    "          ('PREV_BIRTH_PRETERM', typ.IntegerType())\n",
    "          ]\n",
    "\n",
    "schema = typ.StructType([\n",
    "    typ.StructField(e[0], e[1], False) for e in labels\n",
    "])\n",
    "\n",
    "births = spark.read.csv(\n",
    "    'births_transformed.csv', header=True, schema=schema)\n",
    "\n",
    "# births = spark.read.csv(\n",
    "#     'births_transformed.csv', header=True, inferSchema=True)\n",
    "births.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfeec47",
   "metadata": {},
   "source": [
    "## 1.1 数据处理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1f6fb3",
   "metadata": {},
   "source": [
    "### PySpark 数据框操作"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cfee9f",
   "metadata": {},
   "source": [
    "#### 统计信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6845d6c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------------+-------------------+\n",
      "|summary|INFANT_ALIVE_AT_REPORT|        BIRTH_PLACE|\n",
      "+-------+----------------------+-------------------+\n",
      "|  count|                 45429|              45429|\n",
      "|   mean|    0.5139668493693456| 1.0556032490259526|\n",
      "| stddev|    0.4998103900865398|0.44711381563465835|\n",
      "|    min|                     0|                  1|\n",
      "|    max|                     1|                  9|\n",
      "+-------+----------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "births.describe('INFANT_ALIVE_AT_REPORT','BIRTH_PLACE').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ee74d9",
   "metadata": {},
   "source": [
    "#### 过滤数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f112b4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "births.filter(births.INFANT_ALIVE_AT_REPORT=='0').show(2)\n",
    "#多个条件过滤\n",
    "births.filter((births.INFANT_ALIVE_AT_REPORT=='0') & (births.BIRTH_PLACE=='1')).show(2)\n",
    "births.filter((births.INFANT_ALIVE_AT_REPORT=='0') | (births.BIRTH_PLACE=='1')).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a829ef1e",
   "metadata": {},
   "source": [
    "#### 排序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "38140042",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+-----------+----------------+------------------+----------+---------+---------+---------+----------------+-----------------+----------------------+------------------+------------+-------------+------------+-------------+------------------+\n",
      "|INFANT_ALIVE_AT_REPORT|BIRTH_PLACE|MOTHER_AGE_YEARS|FATHER_COMBINE_AGE|CIG_BEFORE|CIG_1_TRI|CIG_2_TRI|CIG_3_TRI|MOTHER_HEIGHT_IN|MOTHER_PRE_WEIGHT|MOTHER_DELIVERY_WEIGHT|MOTHER_WEIGHT_GAIN|DIABETES_PRE|DIABETES_GEST|HYP_TENS_PRE|HYP_TENS_GEST|PREV_BIRTH_PRETERM|\n",
      "+----------------------+-----------+----------------+------------------+----------+---------+---------+---------+----------------+-----------------+----------------------+------------------+------------+-------------+------------+-------------+------------------+\n",
      "|                     1|          1|              28|                28|         0|        0|        0|        0|              62|              120|                   148|                28|           0|            0|           0|            0|                 0|\n",
      "+----------------------+-----------+----------------+------------------+----------+---------+---------+---------+----------------+-----------------+----------------------+------------------+------------+-------------+------------+-------------+------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "births.orderBy('INFANT_ALIVE_AT_REPORT',ascending=False).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2ef1f63a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|INFANT_ALIVE_AT_REPORT|\n",
      "+----------------------+\n",
      "|                     1|\n",
      "|                     0|\n",
      "+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "births.select('INFANT_ALIVE_AT_REPORT').distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d08e72",
   "metadata": {},
   "source": [
    "#### 分组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b7085d8d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+---------------------------+---------------------+-----------------------+---------------+--------------+--------------+--------------+---------------------+----------------------+---------------------------+-----------------------+-----------------+------------------+-----------------+------------------+-----------------------+\n",
      "|INFANT_ALIVE_AT_REPORT|sum(INFANT_ALIVE_AT_REPORT)|sum(MOTHER_AGE_YEARS)|sum(FATHER_COMBINE_AGE)|sum(CIG_BEFORE)|sum(CIG_1_TRI)|sum(CIG_2_TRI)|sum(CIG_3_TRI)|sum(MOTHER_HEIGHT_IN)|sum(MOTHER_PRE_WEIGHT)|sum(MOTHER_DELIVERY_WEIGHT)|sum(MOTHER_WEIGHT_GAIN)|sum(DIABETES_PRE)|sum(DIABETES_GEST)|sum(HYP_TENS_PRE)|sum(HYP_TENS_GEST)|sum(PREV_BIRTH_PRETERM)|\n",
      "+----------------------+---------------------------+---------------------+-----------------------+---------------+--------------+--------------+--------------+---------------------+----------------------+---------------------------+-----------------------+-----------------+------------------+-----------------+------------------+-----------------------+\n",
      "|                     1|                      23349|               662730|                 924112|          31150|         19083|         14467|         12702|              1509958|               4238103|                    4818962|                 777340|              171|              1335|              431|              1214|                    664|\n",
      "|                     0|                          0|               622839|                1099739|          33722|         22064|         17446|         13648|              1448419|               5506345|                    5340148|                 619146|              377|               643|              650|               913|                   1677|\n",
      "+----------------------+---------------------------+---------------------+-----------------------+---------------+--------------+--------------+--------------+---------------------+----------------------+---------------------------+-----------------------+-----------------+------------------+-----------------+------------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "births.groupby('INFANT_ALIVE_AT_REPORT').sum().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde6dd42",
   "metadata": {},
   "source": [
    "#### 执行SQL查询"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ccb4b81b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+-----------+----------------+------------------+----------+---------+---------+---------+----------------+-----------------+----------------------+------------------+------------+-------------+------------+-------------+------------------+\n",
      "|INFANT_ALIVE_AT_REPORT|BIRTH_PLACE|MOTHER_AGE_YEARS|FATHER_COMBINE_AGE|CIG_BEFORE|CIG_1_TRI|CIG_2_TRI|CIG_3_TRI|MOTHER_HEIGHT_IN|MOTHER_PRE_WEIGHT|MOTHER_DELIVERY_WEIGHT|MOTHER_WEIGHT_GAIN|DIABETES_PRE|DIABETES_GEST|HYP_TENS_PRE|HYP_TENS_GEST|PREV_BIRTH_PRETERM|\n",
      "+----------------------+-----------+----------------+------------------+----------+---------+---------+---------+----------------+-----------------+----------------------+------------------+------------+-------------+------------+-------------+------------------+\n",
      "|                     0|          1|              29|                99|         0|        0|        0|        0|              99|              999|                   999|                99|           0|            0|           0|            0|                 0|\n",
      "+----------------------+-----------+----------------+------------------+----------+---------+---------+---------+----------------+-----------------+----------------------+------------------+------------+-------------+------------+-------------+------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "births.registerTempTable('birth_table')\n",
    "sqlContext=SQLContext(spark)\n",
    "sqlContext.sql('select * from birth_table').show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc64a45",
   "metadata": {},
   "source": [
    "#### 离散连续变量\n",
    "我们常常需要处理高度非线性连续特征，很难只用一个系数来供给模型。这种情况下，可能难以用一个系数来解释这样的特征与目标之间的关系。有时候，将值划分成分类级别是很有用的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "10d1728e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|DIABETES_GEST|\n",
      "+-------------+\n",
      "|0            |\n",
      "|0            |\n",
      "|0            |\n",
      "|0            |\n",
      "|0            |\n",
      "+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 使用QuantileDiscretizer模型将连续变量分为五个分类级别\n",
    "discretizer = ft.QuantileDiscretizer(\n",
    "    numBuckets=5, \n",
    "    inputCol='DIABETES_GEST',\n",
    "    outputCol='discritized'\n",
    ")\n",
    "\n",
    "data_discretized = discretizer.fit(birth_train).transform(birth_test)\n",
    "data_discretized.select(\"DIABETES_GEST\").show(5,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d427e0c",
   "metadata": {},
   "source": [
    "####  标准化连续变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "a30ff342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+-----------+----------------+------------------+----------+---------+---------+---------+----------------+-----------------+----------------------+------------------+------------+-------------+------------+-------------+------------------+---------------+----------------------------+--------------------+\n",
      "|INFANT_ALIVE_AT_REPORT|BIRTH_PLACE|MOTHER_AGE_YEARS|FATHER_COMBINE_AGE|CIG_BEFORE|CIG_1_TRI|CIG_2_TRI|CIG_3_TRI|MOTHER_HEIGHT_IN|MOTHER_PRE_WEIGHT|MOTHER_DELIVERY_WEIGHT|MOTHER_WEIGHT_GAIN|DIABETES_PRE|DIABETES_GEST|HYP_TENS_PRE|HYP_TENS_GEST|PREV_BIRTH_PRETERM|BIRTH_PLACE_INT|DIABETES_GEST_continuous_vec|           normlized|\n",
      "+----------------------+-----------+----------------+------------------+----------+---------+---------+---------+----------------+-----------------+----------------------+------------------+------------+-------------+------------+-------------+------------------+---------------+----------------------------+--------------------+\n",
      "|                     0|          1|              12|                99|         0|        0|        0|        0|              60|              154|                   154|                 0|           0|            0|           0|            0|                 0|              1|                       [0.0]|[-0.2154733332549...|\n",
      "|                     0|          1|              12|                99|         0|        0|        0|        0|              62|              145|                   152|                 7|           0|            0|           0|            0|                 0|              1|                       [0.0]|[-0.2154733332549...|\n",
      "|                     0|          1|              13|                99|         0|        0|        0|        0|              62|              218|                   240|                22|           0|            0|           0|            0|                 0|              1|                       [0.0]|[-0.2154733332549...|\n",
      "|                     0|          1|              13|                99|         0|        0|        0|        0|              64|              125|                   143|                18|           0|            0|           0|            0|                 0|              1|                       [0.0]|[-0.2154733332549...|\n",
      "+----------------------+-----------+----------------+------------------+----------+---------+---------+---------+----------------+-----------------+----------------------+------------------+------------+-------------+------------+-------------+------------------+---------------+----------------------------+--------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 标准化连续变量\n",
    "# 首先，要创建一个向量代表连续变量（因为它只是一个float）\n",
    "outputCol='DIABETES_GEST_continuous_vec'\n",
    "vectorizer = ft.VectorAssembler(inputCols=['DIABETES_GEST'],\n",
    "                               outputCol=outputCol)\n",
    "normlizer = ft.StandardScaler(inputCol=vectorizer.getOutputCol(),\n",
    "                             outputCol='normlized',\n",
    "                             withMean=True,\n",
    "                             withStd=True)\n",
    "pipeline = Pipeline(stages=[vectorizer, normlizer])\n",
    "data_standardized = pipeline.fit(birth_train).transform(birth_train)\n",
    "data_standardized.show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f2a890",
   "metadata": {},
   "source": [
    "## 1.2 建模"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc720ab",
   "metadata": {},
   "source": [
    "### 1.2.1 创建转换器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c4b4b53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建转换器、评估器\n",
    "import  pyspark.ml.feature as ft\n",
    "#adding a column or replacing the existing column that has the same name\n",
    "births = births.withColumn('BIRTH_PLACE_INT', births['BIRTH_PLACE']\\\n",
    "    .cast(typ.IntegerType()))\n",
    "\n",
    "# birth place使用one-hot编码\n",
    "encoder = ft.OneHotEncoder(inputCol='BIRTH_PLACE_INT',\n",
    "                           outputCol='BIRTH_PLACE_VEC')\n",
    "\n",
    "# 创建单一的列将所有特征整合在一起\n",
    "# encoder.getOutputCol() 输出列名 'BIRTH_PLACE_VEC'\n",
    "featuresCreator = ft.VectorAssembler(\n",
    "    inputCols=[col[0] for col in labels[2:]] + [encoder.getOutputCol()],\n",
    "    outputCol='features'\n",
    ")\n",
    "\n",
    "# 创建一个评估器\n",
    "import pyspark.ml.classification as cl\n",
    "\n",
    "logistic = cl.LogisticRegression(maxIter=10,\n",
    "                                regParam=0.01,\n",
    "                                featuresCol=featuresCreator.getOutputCol(),\n",
    "                                labelCol='INFANT_ALIVE_AT_REPORT')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61b5ca2",
   "metadata": {},
   "source": [
    "###  1.2.2 创建一个管道、拟合模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0b4318b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建一个管道\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipeline = Pipeline(stages=[encoder, featuresCreator, logistic])\n",
    "\n",
    "# 拟合模型\n",
    "birth_train, birth_test = births.randomSplit([0.7,0.3],seed=123)\n",
    "\n",
    "model = pipeline.fit(birth_train)\n",
    "test_model = model.transform(birth_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "68f38d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dec=cl.DecisionTreeClassifier(featuresCol=featuresCreator.getOutputCol(),\n",
    "                                labelCol='INFANT_ALIVE_AT_REPORT')\n",
    "\n",
    "pipeline = Pipeline(stages=[encoder, featuresCreator, dec])\n",
    "\n",
    "model = pipeline.fit(birth_train)\n",
    "test_model = model.transform(birth_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "e4cee3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt=cl.GBTClassifier(featuresCol=featuresCreator.getOutputCol(),\n",
    "                                labelCol='INFANT_ALIVE_AT_REPORT')\n",
    "pipeline = Pipeline(stages=[encoder, featuresCreator, gbt])\n",
    "\n",
    "model = pipeline.fit(birth_train)\n",
    "test_model = model.transform(birth_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "041bf789",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf=cl.RandomForestClassifier(featuresCol=featuresCreator.getOutputCol(),\n",
    "                                labelCol='INFANT_ALIVE_AT_REPORT')\n",
    "pipeline = Pipeline(stages=[encoder, featuresCreator, gbt])\n",
    "\n",
    "model = pipeline.fit(birth_train)\n",
    "test_model = model.transform(birth_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5aacc75",
   "metadata": {},
   "source": [
    " ### 1.2.3 评估模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "55cb8e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7388890714322204\n",
      "0.7105608237198974\n"
     ]
    }
   ],
   "source": [
    "# 评估模型性能\n",
    "import pyspark.ml.evaluation as ev\n",
    "\n",
    "evaluator = ev.BinaryClassificationEvaluator(\n",
    "    rawPredictionCol='probability',\n",
    "    labelCol='INFANT_ALIVE_AT_REPORT'\n",
    ")\n",
    "\n",
    "print(evaluator.evaluate(test_model, {evaluator.metricName:'areaUnderROC'}))\n",
    "print(evaluator.evaluate(test_model, {evaluator.metricName:'areaUnderPR'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011bb18b",
   "metadata": {},
   "source": [
    "### 1.2.4 保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9b7bcb0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(INFANT_ALIVE_AT_REPORT=0, BIRTH_PLACE='1', MOTHER_AGE_YEARS=13, FATHER_COMBINE_AGE=99, CIG_BEFORE=0, CIG_1_TRI=0, CIG_2_TRI=0, CIG_3_TRI=0, MOTHER_HEIGHT_IN=57, MOTHER_PRE_WEIGHT=100, MOTHER_DELIVERY_WEIGHT=108, MOTHER_WEIGHT_GAIN=8, DIABETES_PRE=0, DIABETES_GEST=0, HYP_TENS_PRE=0, HYP_TENS_GEST=0, PREV_BIRTH_PRETERM=0, BIRTH_PLACE_INT=1, BIRTH_PLACE_VEC=SparseVector(9, {1: 1.0}), features=SparseVector(24, {0: 13.0, 1: 99.0, 6: 57.0, 7: 100.0, 8: 108.0, 9: 8.0, 16: 1.0}), rawPrediction=DenseVector([0.7027, -0.7027]), probability=DenseVector([0.6688, 0.3312]), prediction=0.0)]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 保存模型pipeline\n",
    "pipelinePath = './infant_oneHotEncoder_Logistic_Pipeline'\n",
    "pipeline.write().overwrite().save(pipelinePath)\n",
    "\n",
    "# 重载模型pipeline\n",
    "loadedPipeline = Pipeline.load(pipelinePath)\n",
    "loadedPipeline.fit(birth_train).transform(birth_test).take(1)\n",
    "\n",
    "# 保存模型\n",
    "from pyspark.ml import PipelineModel\n",
    "\n",
    "modelPath = './infant_oneHotEncoder_LogisticPipelineModel'\n",
    "model.write().overwrite().save(modelPath)\n",
    "\n",
    "# 载入模型\n",
    "loadedPipelineModel = PipelineModel.load(modelPath)\n",
    "test_reloadedModel = loadedPipelineModel.transform(birth_test)\n",
    "test_reloadedModel.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf2f470",
   "metadata": {},
   "source": [
    "### 1.2.5 超参调优"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7bf3ca",
   "metadata": {},
   "source": [
    "#### gird search 和train-validation splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9c2e1d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7396088056359407\n",
      "0.712054027633397\n"
     ]
    }
   ],
   "source": [
    "# 超参调优：grid search和train-validation splitting \n",
    "\n",
    "# 网格搜索\n",
    "import pyspark.ml.tuning as tune\n",
    "\n",
    "logistic = cl.LogisticRegression(labelCol='INFANT_ALIVE_AT_REPORT')\n",
    "grid = tune.ParamGridBuilder()\\\n",
    "    .addGrid(logistic.maxIter, [5,10,50])\\\n",
    "    .addGrid(logistic.regParam, [0.01,0.05,0.3])\\\n",
    "    .build()\n",
    "\n",
    "evaluator = ev.BinaryClassificationEvaluator(\n",
    "    rawPredictionCol='probability',\n",
    "    labelCol='INFANT_ALIVE_AT_REPORT'\n",
    ")\n",
    "\n",
    "# 使用K-Fold交叉验证评估各种参数的模型\n",
    "cv = tune.CrossValidator(\n",
    "    estimator=logistic,\n",
    "    estimatorParamMaps=grid,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=3\n",
    ")\n",
    "\n",
    "# 创建一个构建特征的pipeline\n",
    "pipeline = Pipeline(stages=[encoder, featuresCreator])\n",
    "birth_train, birth_test = births.randomSplit([0.7,0.3],seed=123) # 重新打开数据进行处理\n",
    "data_transformer = pipeline.fit(birth_train)\n",
    "data_test = data_transformer.transform(birth_test)\n",
    "\n",
    "\n",
    "# cvModel 返回估计的最佳模型\n",
    "cvModel = cv.fit(data_transformer.transform(birth_train))\n",
    "results = cvModel.transform(data_test)\n",
    "\n",
    "print(evaluator.evaluate(results, {evaluator.metricName:'areaUnderROC'}))\n",
    "print(evaluator.evaluate(results, {evaluator.metricName:'areaUnderPR'}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a586b4",
   "metadata": {},
   "source": [
    "#### 查看模型最佳参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "3047bba1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[({'maxIter': 50, 'regParam': 0.01}, 0.7383169326246161),\n",
       " ({'maxIter': 10, 'regParam': 0.01}, 0.7359418605087911),\n",
       " ({'maxIter': 50, 'regParam': 0.05}, 0.7329773273482909),\n",
       " ({'maxIter': 10, 'regParam': 0.05}, 0.7311161703881676),\n",
       " ({'maxIter': 10, 'regParam': 0.3}, 0.7222756507888913),\n",
       " ({'maxIter': 50, 'regParam': 0.3}, 0.7192552175136916),\n",
       " ({'maxIter': 5, 'regParam': 0.01}, 0.7159320378442128),\n",
       " ({'maxIter': 5, 'regParam': 0.05}, 0.7153360228334891),\n",
       " ({'maxIter': 5, 'regParam': 0.3}, 0.7149080726681669)]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_maps = cvModel.getEstimatorParamMaps()\n",
    "eval_metrics = cvModel.avgMetrics\n",
    "\n",
    "param_res = []\n",
    "\n",
    "for params, metric in zip(param_maps, eval_metrics):\n",
    "    param_metric = {}\n",
    "    for key, param_val in zip(params.keys(), params.values()):\n",
    "        param_metric[key.name]=param_val\n",
    "    param_res.append((param_metric, metric))\n",
    "\n",
    "sorted(param_res, key=lambda x:x[1], reverse=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a73d6b5",
   "metadata": {},
   "source": [
    "#### 使用1-Fold的交叉验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "3f6191f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6129963621116077\n",
      "0.5799248094822335\n"
     ]
    }
   ],
   "source": [
    "# Train-validation划分\n",
    "\n",
    "# 使用卡方检验选择特征\n",
    "selector = ft.ChiSqSelector(\n",
    "    numTopFeatures=5,\n",
    "    featuresCol=featuresCreator.getOutputCol(),\n",
    "    outputCol='selectedFeatures',\n",
    "    labelCol='INFANT_ALIVE_AT_REPORT'\n",
    ")\n",
    "\n",
    "logistic = cl.LogisticRegression(labelCol='INFANT_ALIVE_AT_REPORT',\n",
    "                                featuresCol='selectedFeatures')\n",
    "\n",
    "pipeline = Pipeline(stages=[encoder, featuresCreator, selector])\n",
    "data_transformer = pipeline.fit(birth_train)\n",
    "\n",
    "tvs = tune.TrainValidationSplit(estimator=logistic,\n",
    "                               estimatorParamMaps=grid,\n",
    "                               evaluator=evaluator,\n",
    "                                trainRatio=0.75\n",
    "                               )\n",
    "\n",
    "tvsModel = tvs.fit(data_transformer.transform(birth_train))\n",
    "data_test = data_transformer.transform(birth_test)\n",
    "results = tvsModel.transform(data_test)\n",
    "\n",
    "print(evaluator.evaluate(results, {evaluator.metricName:'areaUnderROC'}))\n",
    "print(evaluator.evaluate(results, {evaluator.metricName:'areaUnderPR'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2818eb",
   "metadata": {},
   "source": [
    "其他案例"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7891367",
   "metadata": {},
   "source": [
    "# 2 聚类"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a249725",
   "metadata": {},
   "source": [
    "数据来源：https://www.kaggle.com/arjunbhasin2013/ccdata\n",
    "该数据集由超过6个月的9K名活跃信用卡持卡人及其交易和账户属性组成。其想法是制定一个客户细分的营销策略。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00f0f715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CUST_ID: string (nullable = true)\n",
      " |-- BALANCE: double (nullable = true)\n",
      " |-- BALANCE_FREQUENCY: double (nullable = true)\n",
      " |-- PURCHASES: double (nullable = true)\n",
      " |-- ONEOFF_PURCHASES: double (nullable = true)\n",
      " |-- INSTALLMENTS_PURCHASES: double (nullable = true)\n",
      " |-- CASH_ADVANCE: double (nullable = true)\n",
      " |-- PURCHASES_FREQUENCY: double (nullable = true)\n",
      " |-- ONEOFF_PURCHASES_FREQUENCY: double (nullable = true)\n",
      " |-- PURCHASES_INSTALLMENTS_FREQUENCY: double (nullable = true)\n",
      " |-- CASH_ADVANCE_FREQUENCY: double (nullable = true)\n",
      " |-- CASH_ADVANCE_TRX: integer (nullable = true)\n",
      " |-- PURCHASES_TRX: integer (nullable = true)\n",
      " |-- CREDIT_LIMIT: double (nullable = true)\n",
      " |-- PAYMENTS: double (nullable = true)\n",
      " |-- MINIMUM_PAYMENTS: double (nullable = true)\n",
      " |-- PRC_FULL_PAYMENT: double (nullable = true)\n",
      " |-- TENURE: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    " \n",
    "spark = SparkSession.builder.appName('Clustering using K-Means').getOrCreate()\n",
    " \n",
    "data_customer=spark.read.csv('./CC GENERAL.csv', header=True, inferSchema=True)\n",
    " \n",
    "data_customer.printSchema()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "0dbbc432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+-----------------+---------+----------------+----------------------+------------+-------------------+--------------------------+--------------------------------+----------------------+----------------+-------------+------------+-----------+----------------+----------------+------+--------------------+\n",
      "|CUST_ID|    BALANCE|BALANCE_FREQUENCY|PURCHASES|ONEOFF_PURCHASES|INSTALLMENTS_PURCHASES|CASH_ADVANCE|PURCHASES_FREQUENCY|ONEOFF_PURCHASES_FREQUENCY|PURCHASES_INSTALLMENTS_FREQUENCY|CASH_ADVANCE_FREQUENCY|CASH_ADVANCE_TRX|PURCHASES_TRX|CREDIT_LIMIT|   PAYMENTS|MINIMUM_PAYMENTS|PRC_FULL_PAYMENT|TENURE|            features|\n",
      "+-------+-----------+-----------------+---------+----------------+----------------------+------------+-------------------+--------------------------+--------------------------------+----------------------+----------------+-------------+------------+-----------+----------------+----------------+------+--------------------+\n",
      "| C10001|  40.900749|         0.818182|     95.4|             0.0|                  95.4|         0.0|           0.166667|                       0.0|                        0.083333|                   0.0|               0|            2|      1000.0| 201.802084|      139.509787|             0.0|    12|[40.900749,0.8181...|\n",
      "| C10002|3202.467416|         0.909091|      0.0|             0.0|                   0.0| 6442.945483|                0.0|                       0.0|                             0.0|                  0.25|               4|            0|      7000.0|4103.032597|     1072.340217|        0.222222|    12|(17,[0,1,5,9,10,1...|\n",
      "+-------+-----------+-----------------+---------+----------------+----------------------+------------+-------------------+--------------------------+--------------------------------+----------------------+----------------+-------------+------------+-----------+----------------+----------------+------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_customer=data_customer.na.drop()\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "data_customer.columns\n",
    " \n",
    "assemble=VectorAssembler(inputCols=[\n",
    " 'BALANCE',\n",
    " 'BALANCE_FREQUENCY',\n",
    " 'PURCHASES',\n",
    " 'ONEOFF_PURCHASES',\n",
    " 'INSTALLMENTS_PURCHASES',\n",
    " 'CASH_ADVANCE',\n",
    " 'PURCHASES_FREQUENCY',\n",
    " 'ONEOFF_PURCHASES_FREQUENCY',\n",
    " 'PURCHASES_INSTALLMENTS_FREQUENCY',\n",
    " 'CASH_ADVANCE_FREQUENCY',\n",
    " 'CASH_ADVANCE_TRX',\n",
    " 'PURCHASES_TRX',\n",
    " 'CREDIT_LIMIT',\n",
    " 'PAYMENTS',\n",
    " 'MINIMUM_PAYMENTS',\n",
    " 'PRC_FULL_PAYMENT',\n",
    " 'TENURE'], outputCol='features')\n",
    " \n",
    "assembled_data=assemble.transform(data_customer)\n",
    " \n",
    "assembled_data.show(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "7fd6f699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+-----------------+---------+----------------+----------------------+------------+-------------------+--------------------------+--------------------------------+----------------------+----------------+-------------+------------+-----------+----------------+----------------+------+--------------------+--------------------+\n",
      "|CUST_ID|    BALANCE|BALANCE_FREQUENCY|PURCHASES|ONEOFF_PURCHASES|INSTALLMENTS_PURCHASES|CASH_ADVANCE|PURCHASES_FREQUENCY|ONEOFF_PURCHASES_FREQUENCY|PURCHASES_INSTALLMENTS_FREQUENCY|CASH_ADVANCE_FREQUENCY|CASH_ADVANCE_TRX|PURCHASES_TRX|CREDIT_LIMIT|   PAYMENTS|MINIMUM_PAYMENTS|PRC_FULL_PAYMENT|TENURE|            features|        standardized|\n",
      "+-------+-----------+-----------------+---------+----------------+----------------------+------------+-------------------+--------------------------+--------------------------------+----------------------+----------------+-------------+------------+-----------+----------------+----------------+------+--------------------+--------------------+\n",
      "| C10001|  40.900749|         0.818182|     95.4|             0.0|                  95.4|         0.0|           0.166667|                       0.0|                        0.083333|                   0.0|               0|            2|      1000.0| 201.802084|      139.509787|             0.0|    12|[40.900749,0.8181...|[0.01951770812869...|\n",
      "| C10002|3202.467416|         0.909091|      0.0|             0.0|                   0.0| 6442.945483|                0.0|                       0.0|                             0.0|                  0.25|               4|            0|      7000.0|4103.032597|     1072.340217|        0.222222|    12|(17,[0,1,5,9,10,1...|(17,[0,1,5,9,10,1...|\n",
      "+-------+-----------+-----------------+---------+----------------+----------------------+------------+-------------------+--------------------------+--------------------------------+----------------------+----------------+-------------+------------+-----------+----------------+----------------+------+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    " \n",
    "scale=StandardScaler(inputCol='features',outputCol='standardized')\n",
    " \n",
    "data_scale=scale.fit(assembled_data)\n",
    "data_scale_output=data_scale.transform(assembled_data)\n",
    " \n",
    "data_scale_output.show(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "9cf5a277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Score: 0.2978057438397949\n",
      "Silhouette Score: 0.28213459315975054\n",
      "Silhouette Score: 0.23556066506177795\n",
      "Silhouette Score: 0.30043360808586705\n",
      "Silhouette Score: 0.22875540553791132\n",
      "Silhouette Score: 0.3107943779144613\n",
      "Silhouette Score: 0.31073584003246785\n",
      "Silhouette Score: 0.3130176826230892\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    " \n",
    "silhouette_score=[]\n",
    " \n",
    "evaluator = ClusteringEvaluator(predictionCol='prediction', featuresCol='standardized', \\\n",
    "                                metricName='silhouette', distanceMeasure='squaredEuclidean')\n",
    " \n",
    "for i in range(2,10):\n",
    " \n",
    "    KMeans_algo=KMeans(featuresCol='standardized', k=i)\n",
    " \n",
    "    KMeans_fit=KMeans_algo.fit(data_scale_output)\n",
    " \n",
    "    output=KMeans_fit.transform(data_scale_output)\n",
    " \n",
    " \n",
    " \n",
    "    score=evaluator.evaluate(output)\n",
    " \n",
    "    silhouette_score.append(score)\n",
    " \n",
    "    print(\"Silhouette Score:\",score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "280c928e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'cos')"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 可视化轮廓分数\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(1,1, figsize =(8,6))\n",
    "ax.plot(range(2,10),silhouette_score)\n",
    "ax.set_xlabel('k')\n",
    "ax.set_ylabel('cos')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1634ff8",
   "metadata": {},
   "source": [
    "# 3 回归"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238fe210",
   "metadata": {},
   "source": [
    "数据集简介  https://datahack.analyticsvidhya.com/contest/black-friday/\n",
    "\n",
    "某零售公司想要了解针对不同类别的各种产品的顾客购买行为（购买量）。他们为上个月选定的大批量产品分享了各种客户的购买汇总。该数据集还包含客户人口统计信息(age, gender, marital status, city_type, stay_in_current_city)，产品详细信息（product_id and product category）以及上个月的purchase_amount总数。现在，他们希望建立一个模型来预测客户对各种产品的购买量，这将有助于他们为不同产品的客户创建个性化的产品。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "b995ca3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- User_ID: integer (nullable = true)\n",
      " |-- Product_ID: string (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Age: string (nullable = true)\n",
      " |-- Occupation: integer (nullable = true)\n",
      " |-- City_Category: string (nullable = true)\n",
      " |-- Stay_In_Current_City_Years: string (nullable = true)\n",
      " |-- Marital_Status: integer (nullable = true)\n",
      " |-- Product_Category_1: integer (nullable = true)\n",
      " |-- Product_Category_2: integer (nullable = true)\n",
      " |-- Product_Category_3: integer (nullable = true)\n",
      " |-- Purchase: integer (nullable = true)\n",
      "\n",
      "550068\n",
      "233599\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    " \n",
    "spark = SparkSession\\\n",
    "    .builder \\\n",
    "    .appName(\"test\") \\\n",
    "    .config(\"spark.some.config.option\", \"setting\") \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "train = spark.read.csv('./BlackFriday/train.csv', header=True, inferSchema=True)\n",
    "test = spark.read.csv('./BlackFriday/test.csv', header=True,  inferSchema=True)\n",
    "train.printSchema()\n",
    "# 查看数据框架中的行数\n",
    "print(train.count())\n",
    "print(test.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "8f89c225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 通过调用drop()方法，可以检查train上非空数值的个数，并进行测试。默认情况下，drop()方法将删除包含任何空值的行。我们还可以通过设置参数“all”,当且仅当该行所有参数都为null时以删除该行。这与pandas上的drop方法类似。\n",
    "train.na.drop('any').count(),test.na.drop(\"any\").count()\n",
    "# 填补\n",
    "train=train.fillna(-1)\n",
    "test=test.fillna(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "489974f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+----------+------+------+-----------------+-------------+--------------------------+-------------------+------------------+------------------+------------------+-----------------+\n",
      "|summary|           User_ID|Product_ID|Gender|   Age|       Occupation|City_Category|Stay_In_Current_City_Years|     Marital_Status|Product_Category_1|Product_Category_2|Product_Category_3|         Purchase|\n",
      "+-------+------------------+----------+------+------+-----------------+-------------+--------------------------+-------------------+------------------+------------------+------------------+-----------------+\n",
      "|  count|            550068|    550068|550068|550068|           550068|       550068|                    550068|             550068|            550068|            550068|            550068|           550068|\n",
      "|   mean|1003028.8424013031|      null|  null|  null|8.076706879876669|         null|         1.468494139793958|0.40965298835780306| 5.404270017525106| 6.419769919355425| 3.145214773446192|9263.968712959126|\n",
      "| stddev|1727.5915855312976|      null|  null|  null|6.522660487341822|         null|        0.9890866807573172|0.49177012631733175| 3.936211369201386| 6.565109781181339| 6.681038828257756|5023.065393820575|\n",
      "|    min|           1000001| P00000142|     F|  0-17|                0|            A|                         0|                  0|                 1|                -1|                -1|               12|\n",
      "|    max|           1006040|  P0099942|     M|   55+|               20|            C|                        4+|                  1|                20|                18|                18|            23961|\n",
      "+-------+------------------+----------+------+------+-----------------+-------------+--------------------------+-------------------+------------------+------------------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 分析数值特征\n",
    "train.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "dbaba720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+\n",
      "|User_ID| Age|\n",
      "+-------+----+\n",
      "|1000001|0-17|\n",
      "|1000001|0-17|\n",
      "|1000001|0-17|\n",
      "|1000001|0-17|\n",
      "|1000002| 55+|\n",
      "+-------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 选择某一列\n",
    "train.select('User_ID',\"Age\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "3ef3190a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3631, 3491)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 分类特征\n",
    "# “train”和“test”中Product_ID的不同类别的数量。这可以通过应用distinct()和count()方法\n",
    "train.select('Product_ID').distinct().count(),test.select('Product_ID').distinct().count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "6642e0dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|Product_ID|\n",
      "+----------+\n",
      "| P00322642|\n",
      "| P00300142|\n",
      "| P00077642|\n",
      "| P00249942|\n",
      "| P00294942|\n",
      "+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 在计算“train”和“test”的不同值的数量后，我们可以看到“train”和“test”有更多的类别。让我们使用相减方法检查Product_ID的类别，这些类别正在\"test\"中，但不在“train”中。我们也可以对所有的分类特征做同样的处理。\n",
    "diff_cat_in_train_test=test.select('Product_ID').subtract(train.select('Product_ID'))\n",
    "diff_cat_in_train_test.distinct().count()\n",
    "diff_cat_in_train_test.distinct().show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "7f188762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|product_id_trans|\n",
      "+----------------+\n",
      "|           765.0|\n",
      "|           183.0|\n",
      "|          1496.0|\n",
      "|           480.0|\n",
      "|           860.0|\n",
      "+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 将分类变量转换为标签\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "plan_indexer=StringIndexer(inputCol='Product_ID',outputCol='product_id_trans')\n",
    "labeller=plan_indexer.fit(train)\n",
    "train1=labeller.transform(train)\n",
    "test1=labeller.transform(test)\n",
    "train1.select('product_id_trans').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bd4655",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  选择特征来构建机器学习模型\n",
    "from pyspark.ml.feature import RFormula\n",
    "# 这个公式中指定依赖和独立的列；我们还必须为为features列和label列指定名称。\n",
    "formula=RFormula(formula=\"Purchase ~ Age+ Occupation +City_Category+Stay_In_Current_City_Years+Product_Category_1+Product_Category_2+ Gender\",\\\n",
    "                 featuresCol='features',labelCol='label')\n",
    "#将这个公式应用到我们的Train1上，并通过这个公式转换Train1,Test1\n",
    "t1=formula.fit(train1)\n",
    "train1=t1.transform(train1)\n",
    "test1=t1.transform(test1)\n",
    "train1.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "71e227df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 机器学习 随机森林回归\n",
    "# train1数据划分为train_cv和test_cv进行交叉验证\n",
    "(train_cv,test_cv)=train1.randomSplit([0.7,0.3])\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "rf=RandomForestRegressor()\n",
    "#建立模型\n",
    "model1=rf.fit(train_cv)\n",
    "predictions=model1.transform(test_cv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "a39b5d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 决策树\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "rf=DecisionTreeRegressor()\n",
    "#建立模型\n",
    "model1=rf.fit(train_cv)\n",
    "predictions=model1.transform(test_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "c924dcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import GBTRegressor\n",
    "rf=GBTRegressor()\n",
    "#建立模型\n",
    "model1=rf.fit(train_cv)\n",
    "predictions=model1.transform(test_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "6ea30af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "rf=LinearRegression()\n",
    "#建立模型\n",
    "model1=rf.fit(train_cv)\n",
    "predictions=model1.transform(test_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "2ad4a1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "evaluator =RegressionEvaluator()\n",
    "mse=evaluator.evaluate(predictions,{evaluator.metricName:\"mse\"})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "c9aa506f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=rf.fit(train1)\n",
    "predictions1=model.transform(test1)\n",
    "df=predictions1.selectExpr(\"User_ID as User_ID\", \"Product_ID as Product_ID\", 'prediction as Purchase')\n",
    "df.toPandas().to_csv('./BlackFriday/submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c361a96",
   "metadata": {},
   "source": [
    "# 5 文本"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990d453a",
   "metadata": {},
   "source": [
    "### 5.1 载入数据集data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c03f493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Dates: string (nullable = true)\n",
      " |-- Category: string (nullable = true)\n",
      " |-- Descript: string (nullable = true)\n",
      " |-- DayOfWeek: string (nullable = true)\n",
      " |-- PdDistrict: string (nullable = true)\n",
      " |-- Resolution: string (nullable = true)\n",
      " |-- Address: string (nullable = true)\n",
      " |-- X: double (nullable = true)\n",
      " |-- Y: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SQLContext,SparkSession\n",
    "from pyspark import SparkContext\n",
    "\n",
    "spark = SparkSession.builder.appName('crime classification').getOrCreate()\n",
    " \n",
    "data=spark.read.csv('./San-Francisco-Crime-Classification/train.csv', header=True, inferSchema=True)\n",
    " \n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78826eee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Category', 'Descript']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drop_list = ['Dates', 'DayOfWeek', 'PdDistrict', 'Resolution', 'Address', 'X', 'Y']\n",
    "\n",
    "data = data.select([column for column in data.columns if column not in drop_list])\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81de8866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+\n",
      "|      Category|            Descript|\n",
      "+--------------+--------------------+\n",
      "|      WARRANTS|      WARRANT ARREST|\n",
      "|OTHER OFFENSES|TRAFFIC VIOLATION...|\n",
      "|OTHER OFFENSES|TRAFFIC VIOLATION...|\n",
      "| LARCENY/THEFT|GRAND THEFT FROM ...|\n",
      "| LARCENY/THEFT|GRAND THEFT FROM ...|\n",
      "+--------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1211f54b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|            Category| count|\n",
      "+--------------------+------+\n",
      "|       LARCENY/THEFT|174900|\n",
      "|      OTHER OFFENSES|126182|\n",
      "|        NON-CRIMINAL| 92304|\n",
      "|             ASSAULT| 76876|\n",
      "|       DRUG/NARCOTIC| 53971|\n",
      "|       VEHICLE THEFT| 53781|\n",
      "|           VANDALISM| 44725|\n",
      "|            WARRANTS| 42214|\n",
      "|            BURGLARY| 36755|\n",
      "|      SUSPICIOUS OCC| 31414|\n",
      "|      MISSING PERSON| 25989|\n",
      "|             ROBBERY| 23000|\n",
      "|               FRAUD| 16679|\n",
      "|FORGERY/COUNTERFE...| 10609|\n",
      "|     SECONDARY CODES|  9985|\n",
      "|         WEAPON LAWS|  8555|\n",
      "|        PROSTITUTION|  7484|\n",
      "|            TRESPASS|  7326|\n",
      "|     STOLEN PROPERTY|  4540|\n",
      "|SEX OFFENSES FORC...|  4388|\n",
      "+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# by top 20 categories\n",
    "data.groupBy(\"Category\") \\\n",
    "    .count() \\\n",
    "    .orderBy(col(\"count\").desc()) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a35de9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            Descript|count|\n",
      "+--------------------+-----+\n",
      "|GRAND THEFT FROM ...|60022|\n",
      "|       LOST PROPERTY|31729|\n",
      "|             BATTERY|27441|\n",
      "|   STOLEN AUTOMOBILE|26897|\n",
      "|DRIVERS LICENSE, ...|26839|\n",
      "|      WARRANT ARREST|23754|\n",
      "|SUSPICIOUS OCCURR...|21891|\n",
      "|AIDED CASE, MENTA...|21497|\n",
      "|PETTY THEFT FROM ...|19771|\n",
      "|MALICIOUS MISCHIE...|17789|\n",
      "|   TRAFFIC VIOLATION|16471|\n",
      "|PETTY THEFT OF PR...|16196|\n",
      "|MALICIOUS MISCHIE...|15957|\n",
      "|THREATS AGAINST LIFE|14716|\n",
      "|      FOUND PROPERTY|12146|\n",
      "|ENROUTE TO OUTSID...|11470|\n",
      "|GRAND THEFT OF PR...|11010|\n",
      "|POSSESSION OF NAR...|10050|\n",
      "|PETTY THEFT FROM ...|10029|\n",
      "|PETTY THEFT SHOPL...| 9571|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.groupBy(\"Descript\") \\\n",
    "    .count() \\\n",
    "    .orderBy(col(\"count\").desc()) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0368b303",
   "metadata": {},
   "source": [
    "### 5.2 对犯罪描述进行分词"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f5b8b7",
   "metadata": {},
   "source": [
    "对Descript分词，先切分单词，再删除停用词\n",
    "流程和scikit-learn版本的很相似，\n",
    "\n",
    "包含3个步骤：\n",
    "\n",
    "1.regexTokenizer: 利用正则切分单词\n",
    "\n",
    "2.stopwordsRemover: 移除停用词\n",
    "\n",
    "3.countVectors: 构建词频向量\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c60b01b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "'''\n",
    "RegexTokenizer：基于正则的方式进行文档切分成单词组\n",
    "inputCol: 输入字段\n",
    "outputCol: 输出字段\n",
    "pattern： 匹配模式，根据匹配到的内容切分单词\n",
    "'''\n",
    "# regular expression tokenizer\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"Descript\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "\n",
    "# stop words\n",
    "add_stopwords = [\"http\",\"https\",\"amp\",\"rt\",\"t\",\"c\",\"the\"] # standard stop words\n",
    "\n",
    "stopwordsRemover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\").setStopWords(add_stopwords)\n",
    "\n",
    "'''StringIndexer\n",
    "StringIndexer将一列字符串label编码为一列索引号，根据label出现的频率排序，最频繁出现的label的index为0\n",
    "该例子中，label会被编码成从0-32的整数，最频繁的label被编码成0'''\n",
    "label_stringIdx = StringIndexer(inputCol = \"Category\", outputCol = \"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8621932",
   "metadata": {},
   "source": [
    "对分词后的词频率排序，最频繁出现的设置为0\n",
    "\n",
    "\n",
    "\n",
    "Pipeline是基于DataFrame的高层API，可以方便用户构建和调试机器学习流水线，可以使得多个机器学习算法顺序执行，达到高效的数据处理的目的。\n",
    "\n",
    "fit():将DataFrame转换成一个Transformer的算法，将label列转化为特征向量\n",
    "transform(): 将特征向量作为新列添加到DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7102a49b",
   "metadata": {},
   "source": [
    "### 5.3 模型训练"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2deda83b",
   "metadata": {},
   "source": [
    "#### 以词频作为特征\n",
    "\n",
    "模型在测试集上预测和打分，查看10个预测概率值最高的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05861218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "|      Category|            Descript|               words|            filtered|            features|label|\n",
      "+--------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "|      WARRANTS|      WARRANT ARREST|   [warrant, arrest]|   [warrant, arrest]|(809,[17,32],[1.0...|  7.0|\n",
      "|OTHER OFFENSES|TRAFFIC VIOLATION...|[traffic, violati...|[traffic, violati...|(809,[11,17,35],[...|  1.0|\n",
      "|OTHER OFFENSES|TRAFFIC VIOLATION...|[traffic, violati...|[traffic, violati...|(809,[11,17,35],[...|  1.0|\n",
      "| LARCENY/THEFT|GRAND THEFT FROM ...|[grand, theft, fr...|[grand, theft, fr...|(809,[0,2,3,4,6],...|  0.0|\n",
      "| LARCENY/THEFT|GRAND THEFT FROM ...|[grand, theft, fr...|[grand, theft, fr...|(809,[0,2,3,4,6],...|  0.0|\n",
      "+--------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "'''CountVectorizer：构建词频向量\n",
    "covabSize: 限制的词频数\n",
    "minDF：如果是float，则表示出现的百分比小于minDF,不会被当做关键词\n",
    "如果是int，则表示出现是次数小于minDF，不会被当做关键词'''\n",
    "# bag of words count\n",
    "countVectors = CountVectorizer(inputCol=\"filtered\", outputCol=\"features\", vocabSize=10000, minDF=5)\n",
    "pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover, countVectors, label_stringIdx])\n",
    "\n",
    "# Fit the pipeline to training documents.\n",
    "pipelineFit = pipeline.fit(data)\n",
    "dataset = pipelineFit.transform(data)\n",
    "dataset.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83fc1f7e",
   "metadata": {},
   "source": [
    "#### 以Word2Vec作为特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b318bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.ml.feature as ft\n",
    "word2Vec = ft.Word2Vec(vectorSize=3, minCount=0, inputCol=\"filtered\", outputCol=\"features\")\n",
    "pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover, word2Vec, label_stringIdx])\n",
    "\n",
    "# Fit the pipeline to training documents.\n",
    "pipelineFit = pipeline.fit(data)\n",
    "dataset = pipelineFit.transform(data)\n",
    "dataset.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b5646c",
   "metadata": {},
   "source": [
    "#### 以TF-ID作为特征，利用逻辑回归进行分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c682e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "\n",
    "# Add HashingTF and IDF to transformation\n",
    "hashingTF = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\", numFeatures=10000)\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\", minDocFreq=5) #minDocFreq: remove sparse terms\n",
    "\n",
    "# Redo Pipeline\n",
    "pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover, hashingTF, idf, label_stringIdx])\n",
    "pipelineFit = pipeline.fit(data)\n",
    "dataset = pipelineFit.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5a3d1ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+-------------+------------------------------+-----+----------+\n",
      "|                      Descript|     Category|                   probability|label|prediction|\n",
      "+------------------------------+-------------+------------------------------+-----+----------+\n",
      "|THEFT, BICYCLE, <$50, SERIA...|LARCENY/THEFT|[0.8726871815697533,0.02067...|  0.0|       0.0|\n",
      "|THEFT, BICYCLE, <$50, NO SE...|LARCENY/THEFT|[0.8726871815697531,0.02067...|  0.0|       0.0|\n",
      "|THEFT, BICYCLE, <$50, NO SE...|LARCENY/THEFT|[0.8726871815697531,0.02067...|  0.0|       0.0|\n",
      "|THEFT, BICYCLE, <$50, NO SE...|LARCENY/THEFT|[0.8726871815697531,0.02067...|  0.0|       0.0|\n",
      "|THEFT, BICYCLE, <$50, NO SE...|LARCENY/THEFT|[0.8726871815697531,0.02067...|  0.0|       0.0|\n",
      "|THEFT, BICYCLE, <$50, NO SE...|LARCENY/THEFT|[0.8726871815697531,0.02067...|  0.0|       0.0|\n",
      "|THEFT, BICYCLE, <$50, NO SE...|LARCENY/THEFT|[0.8726871815697531,0.02067...|  0.0|       0.0|\n",
      "|THEFT, BICYCLE, <$50, NO SE...|LARCENY/THEFT|[0.8726871815697531,0.02067...|  0.0|       0.0|\n",
      "|THEFT, BICYCLE, <$50, NO SE...|LARCENY/THEFT|[0.8726871815697531,0.02067...|  0.0|       0.0|\n",
      "|THEFT, BICYCLE, <$50, NO SE...|LARCENY/THEFT|[0.8726871815697531,0.02067...|  0.0|       0.0|\n",
      "+------------------------------+-------------+------------------------------+-----+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed = 100)\n",
    "# Build the model\n",
    "lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\n",
    "\n",
    "# Train model with Training Data\n",
    "lrModel = lr.fit(trainingData)\n",
    "predictions = lrModel.transform(testData)\n",
    "\n",
    "predictions.filter(predictions['prediction'] == 0) \\\n",
    "    .select(\"Descript\",\"Category\",\"probability\",\"label\",\"prediction\") \\\n",
    "    .orderBy(\"probability\", ascending=False) \\\n",
    "    .show(n = 10, truncate = 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6d59dd",
   "metadata": {},
   "source": [
    "### 5.4 模型评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de21eee8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9716071305423885"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "evaluator.evaluate(predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e5df84",
   "metadata": {},
   "source": [
    "### 5.5 交叉验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be85342b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 环境问题运行不了\n",
    "# pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover, countVectors, label_stringIdx])\n",
    "# pipelineFit = pipeline.fit(data)\n",
    "# dataset = pipelineFit.transform(data)\n",
    "# (trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed = 100)\n",
    "\n",
    "# # Build the model\n",
    "# lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\n",
    "\n",
    "# from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "# # Create ParamGrid for Cross Validation\n",
    "# paramGrid = (ParamGridBuilder()\n",
    "#              .addGrid(lr.regParam, [0.1, 0.3, 0.5]) # regularization parameter\n",
    "#              .addGrid(lr.elasticNetParam, [0.0, 0.1, 0.2]) # Elastic Net Parameter (Ridge = 0)\n",
    "# #            .addGrid(model.maxIter, [10, 20, 50]) #Number of iterations\n",
    "# #            .addGrid(idf.numFeatures, [10, 100, 1000]) # Number of features\n",
    "#              .build())\n",
    "\n",
    "# # Create 5-fold CrossValidator\n",
    "# cv = CrossValidator(estimator=lr, \\\n",
    "#                     estimatorParamMaps=paramGrid, \\\n",
    "#                     evaluator=evaluator, \\\n",
    "#                     numFolds=5)\n",
    "\n",
    "# # Run cross validations\n",
    "# cvModel = cv.fit(trainingData)\n",
    "# # this will likely take a fair amount of time because of the amount of models that we're creating and testing\n",
    "\n",
    "# # Use test set here so we can measure the accuracy of our model on new data\n",
    "# predictions = cvModel.transform(testData)\n",
    "\n",
    "# # cvModel uses the best model found from the Cross Validation\n",
    "# # Evaluate best model\n",
    "# evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "# evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fefb8b",
   "metadata": {},
   "source": [
    "#### NaiveBayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccea6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # An error occurred while trying to connect to the Java server (127.0.0.1:46597)\n",
    "# from pyspark.ml.classification import NaiveBayes\n",
    "\n",
    "# # create the trainer and set its parameters\n",
    "# nb = NaiveBayes(smoothing=1)\n",
    "\n",
    "# # train the model\n",
    "# model = nb.fit(trainingData)\n",
    "# predictions = model.transform(testData)\n",
    "# predictions.filter(predictions['prediction'] == 0) \\\n",
    "#     .select(\"Descript\",\"Category\",\"probability\",\"label\",\"prediction\") \\\n",
    "#     .orderBy(\"probability\", ascending=False) \\\n",
    "#     .show(n = 10, truncate = 30)\n",
    "# valuator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "# evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ec622c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d6e79e52",
   "metadata": {},
   "source": [
    "# 6 其他操作"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae15db4",
   "metadata": {},
   "source": [
    "## 特征提取"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd06e6b",
   "metadata": {},
   "source": [
    "### NLP相关特征提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "9e148565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|               input|           input_arr|          input_stop|              nGrams|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|K-fold cross vali...|[k-fold, cross, v...|[k-fold, cross, v...|[k-fold cross, cr...|\n",
      "|CrossValidatorMod...|[crossvalidatormo...|[crossvalidatormo...|[crossvalidatormo...|\n",
      "|Creates a copy of...|[creates, a, copy...|[creates, copy, i...|[creates copy, co...|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_data = spark.createDataFrame([\n",
    "    ['''K-fold cross validation performs model selection by splitting the dataset into a set of non-overlapping \n",
    "    randomly partitioned folds which are used as separate training and test datasets e.g., with k=3 folds, \n",
    "    K-fold cross validation will generate 3 (training, test) dataset pairs, each of which uses 2/3 of the data \n",
    "    for training and 1/3 for testing. Each fold is used as the test set exactly once.'''],\n",
    "    ['''CrossValidatorModel contains the model with the highest average cross-validation metric across folds and\n",
    "    uses this model to transform input data. CrossValidatorModel also tracks the metrics for each param map \n",
    "    evaluated.'''],\n",
    "    ['''Creates a copy of this instance with a randomly generated uid and some extra params. This copies the \n",
    "    underlying bestModel, creates a deep copy of the embedded paramMap, and copies the embedded and extra \n",
    "    parameters over.''']\n",
    "], ['input'])\n",
    "\n",
    "# 将文本拆分成单词\n",
    "tokenizer = ft.RegexTokenizer(inputCol='input',\n",
    "                              outputCol='input_arr',\n",
    "                              pattern='\\s+|[,.\\\"]')\n",
    "\n",
    "# 删掉停用词\n",
    "stopwords = ft.StopWordsRemover(inputCol=tokenizer.getOutputCol(),\n",
    "                               outputCol='input_stop')\n",
    "# 生成ngram词对\n",
    "ngram = ft.NGram(n=2,\n",
    "                inputCol=stopwords.getOutputCol(),\n",
    "                outputCol='nGrams')\n",
    "\n",
    "# 构建特征pipeline\n",
    "pipeline = Pipeline(stages=[tokenizer, stopwords, ngram])\n",
    "\n",
    "data_ngram = pipeline\\\n",
    "    .fit(text_data)\\\n",
    "    .transform(text_data)\n",
    "\n",
    "data_ngram.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abbeeb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import  pyspark.ml.feature as ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6af7f961",
   "metadata": {},
   "outputs": [],
   "source": [
    "birth_train, birth_test = births.randomSplit([0.7,0.3],seed=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "badb0ab0",
   "metadata": {},
   "source": [
    "## 转换器"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101bbec6",
   "metadata": {},
   "source": [
    "### Binarizer\n",
    "根据指定的阈值将连续变量转换为对应的二进制值；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d36d1fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|ONEOFF_PURCHASES_B|\n",
      "+------------------+\n",
      "|               0.0|\n",
      "|               0.0|\n",
      "|               1.0|\n",
      "|               1.0|\n",
      "|               1.0|\n",
      "+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----------------+\n",
      "|ONEOFF_PURCHASES|\n",
      "+----------------+\n",
      "|             0.0|\n",
      "|             0.0|\n",
      "|          773.17|\n",
      "|          1499.0|\n",
      "|            16.0|\n",
      "+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import  pyspark.ml.feature as ft\n",
    "#adding a column or replacing the existing column that has the same name\n",
    "data_customer= data_customer.withColumn('ONEOFF_PURCHASES_INT',data_customer['ONEOFF_PURCHASES']\\\n",
    "    .cast(typ.DoubleType()))\n",
    "\n",
    "encoder = ft.Binarizer(inputCol='ONEOFF_PURCHASES',\n",
    "                           outputCol='ONEOFF_PURCHASES_B',threshold=0.5)\n",
    "binarizedDataFrame=encoder.transform(data_customer)\n",
    "binarizedDataFrame.select('ONEOFF_PURCHASES_B').show(5)\n",
    "data_customer.select('ONEOFF_PURCHASES').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bc41672d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CUST_ID: string (nullable = true)\n",
      " |-- BALANCE: double (nullable = true)\n",
      " |-- BALANCE_FREQUENCY: double (nullable = true)\n",
      " |-- PURCHASES: double (nullable = true)\n",
      " |-- ONEOFF_PURCHASES: double (nullable = true)\n",
      " |-- INSTALLMENTS_PURCHASES: double (nullable = true)\n",
      " |-- CASH_ADVANCE: double (nullable = true)\n",
      " |-- PURCHASES_FREQUENCY: double (nullable = true)\n",
      " |-- ONEOFF_PURCHASES_FREQUENCY: double (nullable = true)\n",
      " |-- PURCHASES_INSTALLMENTS_FREQUENCY: double (nullable = true)\n",
      " |-- CASH_ADVANCE_FREQUENCY: double (nullable = true)\n",
      " |-- CASH_ADVANCE_TRX: integer (nullable = true)\n",
      " |-- PURCHASES_TRX: integer (nullable = true)\n",
      " |-- CREDIT_LIMIT: double (nullable = true)\n",
      " |-- PAYMENTS: double (nullable = true)\n",
      " |-- MINIMUM_PAYMENTS: double (nullable = true)\n",
      " |-- PRC_FULL_PAYMENT: double (nullable = true)\n",
      " |-- TENURE: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_customer.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7d7adf",
   "metadata": {},
   "source": [
    "###  Bucketizer\n",
    "与Binarizer类似，该方法根据阈值列表（分割的参数），将连续变量转换为多项值（即将连续变量离散到指定的范围区间）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5cfc309b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function JavaWrapper.__del__ at 0x7f67dc297280>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/ml/wrapper.py\", line 39, in __del__\n",
      "    if SparkContext._active_spark_context and self._java_obj is not None:\n",
      "AttributeError: 'MaxAbsScaler' object has no attribute '_java_obj'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|ONEOFF_PURCHASES_BK|\n",
      "+-------------------+\n",
      "|                0.0|\n",
      "|                0.0|\n",
      "|                1.0|\n",
      "|                2.0|\n",
      "|                0.0|\n",
      "+-------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----------------+\n",
      "|ONEOFF_PURCHASES|\n",
      "+----------------+\n",
      "|             0.0|\n",
      "|             0.0|\n",
      "|          773.17|\n",
      "|          1499.0|\n",
      "|            16.0|\n",
      "+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encoder = ft.Bucketizer(inputCol='ONEOFF_PURCHASES_INT',\n",
    "                           outputCol='ONEOFF_PURCHASES_BK',splits=[-float(\"inf\"), 100, 1000, float(\"inf\")])\n",
    "binarizedDataFrame=encoder.transform(data_customer)\n",
    "binarizedDataFrame.select('ONEOFF_PURCHASES_BK').show(5)\n",
    "data_customer.select('ONEOFF_PURCHASES').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7b5738",
   "metadata": {},
   "source": [
    "### MaxAbsScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7a26019f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|              Scaled|\n",
      "+--------------------+\n",
      "|               [0.0]|\n",
      "|               [0.0]|\n",
      "|[0.01896826029623...|\n",
      "|[0.0367751234321813]|\n",
      "+--------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 首先，要创建一个向量代表连续变量（因为它只是一个float）\n",
    "outputCol='ONEOFF_PURCHASES_vec'\n",
    "vectorizer = ft.VectorAssembler(inputCols=['ONEOFF_PURCHASES'],\n",
    "                               outputCol=outputCol)\n",
    "scaler = ft.MaxAbsScaler(inputCol=vectorizer.getOutputCol(),\n",
    "                             outputCol='Scaled',)\n",
    "pipeline = Pipeline(stages=[vectorizer, scaler])\n",
    "data_standardized = pipeline.fit(data_customer).transform(data_customer)\n",
    "data_standardized.select('Scaled').show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae666ba",
   "metadata": {},
   "source": [
    "### MinMaxScaler\n",
    ",这与MaxAbsScaler相似，区别在于它将数据缩放到[0.0, 1.0]范围内；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c191cbec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|       minmax_Scaled|\n",
      "+--------------------+\n",
      "|               [0.0]|\n",
      "|               [0.0]|\n",
      "|[0.01896826029623...|\n",
      "|[0.0367751234321813]|\n",
      "+--------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# MinMaxScaler, 这与MaxAbsScaler相似，区别在于它将数据缩放到[0.0, 1.0]范围内；\n",
    "outputCol='ONEOFF_PURCHASES_vec'\n",
    "vectorizer = ft.VectorAssembler(inputCols=['ONEOFF_PURCHASES'],\n",
    "                               outputCol=outputCol)\n",
    "scaler = ft.MinMaxScaler(inputCol=vectorizer.getOutputCol(),\n",
    "                             outputCol='minmax_Scaled',)\n",
    "pipeline = Pipeline(stages=[vectorizer, scaler])\n",
    "data_standardized = pipeline.fit(data_customer).transform(data_customer)\n",
    "data_standardized.select('minmax_Scaled').show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330c78f0",
   "metadata": {},
   "source": [
    "### Normlizer\n",
    "该方法使用p范数将数据缩放为单位范数(默认为L2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "110b29c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|minmax_Scaled|\n",
      "+-------------+\n",
      "|        [0.0]|\n",
      "|        [0.0]|\n",
      "|        [1.0]|\n",
      "|        [1.0]|\n",
      "+-------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "outputCol='ONEOFF_PURCHASES_vec'\n",
    "vectorizer = ft.VectorAssembler(inputCols=['ONEOFF_PURCHASES'],\n",
    "                               outputCol=outputCol)\n",
    "scaler = ft.Normalizer(inputCol=vectorizer.getOutputCol(),\n",
    "                             outputCol='minmax_Scaled',)\n",
    "pipeline = Pipeline(stages=[vectorizer, scaler])\n",
    "data_standardized = pipeline.fit(data_customer).transform(data_customer)\n",
    "data_standardized.select('minmax_Scaled').show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a997845b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
